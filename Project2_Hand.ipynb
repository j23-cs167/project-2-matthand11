{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j23-cs167/project-2-matthand11/blob/main/Project2_Hand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project #2\n",
        "##Name: Matt Hand\n",
        "\n",
        "Proposed Points (out of 25):\n",
        "\n",
        "*Overall*: 24/25\n",
        "\n",
        "***\n",
        "\n",
        "*Problem*: 2/2\n",
        "\n",
        "I thoroughly explained the dataset I chose to use and what I wanted to predict by using the data.\n",
        "\n",
        "*Data Prep*: 2/2\n",
        "\n",
        "I prepared my data thoroughly to make it workable with the four different subsets of data I used, and also normalized the data to make my metrics more accurate.\n",
        "\n",
        "*Research*: 9/10\n",
        "\n",
        "I was able to use three different machine learning models we learned in class to thoroughly evaluate each subset of data, and compare the predictive ability of each subset of statistics and each model. I was also able to create an effective graph to showcase my findings in an interpretable way. I deducted one point off my score, as if I had some additional time I would've liked to have dived deeper into possibly having some different subsets or evaluating the differences more. However, I am overally very pleased with what I was able to accomplish and the conclusions I was able to draw.\n",
        "\n",
        "*Analysis*: 10/10\n",
        "\n",
        "I was able to use my models and the results of my tests to come to a conclusion to the problem I laid out at the beginning of my work. I also was able to effectively evaluate the differences in effectiveness between different machine learning models we discussed, and why these differences were prominent.\n",
        "\n",
        "*Bumps in the Road*: 1/1\n",
        "\n",
        "I didn't really have too many huge issues throughout this project, but I outlined the issues I spent the most time on and how I was able to come up with solutions to solve these isssues."
      ],
      "metadata": {
        "id": "rIepkBIuT_Pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Problem\n",
        "State the problem you are trying to solve with this machine learning experiment. Include a description of the data, where you got the data, and what you're trying to predict.."
      ],
      "metadata": {
        "id": "HbwKLNhzP8YO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this machine learning experiment, I will be analyzing an NCAA basketball dataset from the 2018 season. I got this data from user Andrew Sundberg on Kaggle. The full dataset contained data from each season between 2013 and 2021, but I chose to focus on a season that was prior to the COVID-19 pandemic due to its negative effects on the data from the 2020 and 2021 seasons. Within the dataset there are 23 columns and a row for each team in NCAA Division-1. Each column contains a given statistic for each team, with almost all of these statistics being numerical. This will help to make the dataset highly usable.\n",
        "\n",
        "Through the use of this data, I hope to find out how accurately I am able to predict the amount of wins a team will be able to achieve in a season given different subsets of their stats. By using different subsets of statistics, I hope to be able to conclude which statistics are the most important factors in determining teams' success. Theoretically, the more important a subset of statistic is to teams' success, the more accurately the amount of wins a team achieves could be predicted. This would be useful for NCAA basketball teams to know, as they can focus on improving in certain statistical categories which are the most beneficial to winning games."
      ],
      "metadata": {
        "id": "LwseoUsMmCz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Preparation\n",
        "Explain your data preparation. What did you have to do to get your data in shape for your experiments? Why are you certain that you data is clean and prepared for use in your algorithms?"
      ],
      "metadata": {
        "id": "qR_foVOeQVL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to prepare my data for use, there are a few main steps I needed to complete. First, I needed to get my raw data imported from my Google Drive. Once the entire dataset was loaded in to Colab, I could work on splitting it up into different subsets of statistics. To do this, I made column subsets in four categories: offense, defense, rebounding, and efficiency. Once I had these sectioned off, I created a different set of predictor variables for each subset based on what columns were included in the subsets, and set my target variable to be 'W', the amount of wins. After this, I created sets of training and testing data for each subset using train_test_split from sklearn. Then, I normalized each set of data by using StandardScaler from sklearn's preprocessing package.\n",
        "\n",
        "Once these steps were completed, I was confident that the data was well prepared to be used effectively. By having normalized data with useful subsets divided into training and testing sets, my data was in the necessary shape to use multiple different methods for analysis. I also used the value_counts function to check for any NA data points in the columns I am using, which there were not."
      ],
      "metadata": {
        "id": "aMDJDnW_mOWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and prepare your data here\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "path = '/content/drive/MyDrive/CS167/Datasets/cbb18.csv'\n",
        "cbb = pd.read_csv(path)\n",
        "\n",
        "\n",
        "offense_sub = cbb[['W', 'EFG_O', 'TOR', 'FTR', '2P_O', '3P_O']]\n",
        "defense_sub= cbb[['W', 'EFG_D', 'TORD', 'FTRD', '2P_D', '3P_D']]\n",
        "rebounding_sub= cbb[['W', 'ORB', 'DRB']]\n",
        "efficiency_sub= cbb[['W', 'ADJOE', 'ADJDE']]\n",
        "\n",
        "off_predictors = ['EFG_O', 'TOR', 'FTR','2P_O','3P_O']\n",
        "def_predictors = ['EFG_D', 'TORD', 'FTRD', '2P_D', '3P_D']\n",
        "reb_predictors = ['ORB', 'DRB']\n",
        "eff_predictors = ['ADJOE', 'ADJDE']\n",
        "target = \"W\"\n",
        "off_train_data, off_test_data, off_train_sln, off_test_sln = \\\n",
        "    train_test_split(offense_sub[off_predictors], offense_sub[target], test_size = 0.2, random_state=41)\n",
        "def_train_data, def_test_data, def_train_sln, def_test_sln = \\\n",
        "    train_test_split(defense_sub[def_predictors], defense_sub[target], test_size = 0.2, random_state=41)\n",
        "reb_train_data, reb_test_data, reb_train_sln, reb_test_sln = \\\n",
        "    train_test_split(rebounding_sub[reb_predictors], rebounding_sub[target], test_size = 0.2, random_state=41)\n",
        "eff_train_data, eff_test_data, eff_train_sln, eff_test_sln = \\\n",
        "    train_test_split(efficiency_sub[eff_predictors], efficiency_sub[target], test_size = 0.2, random_state=41)\n",
        "\n",
        "## Normalize offense data\n",
        "off_scaler = preprocessing.StandardScaler().fit(off_train_data)\n",
        "otr_norm = off_scaler.transform(off_train_data)\n",
        "ote_norm = off_scaler.transform(off_test_data)\n",
        "\n",
        "## Normalize defense data\n",
        "def_scaler = preprocessing.StandardScaler().fit(def_train_data)\n",
        "dtr_norm = def_scaler.transform(def_train_data)\n",
        "dte_norm = def_scaler.transform(def_test_data)\n",
        "\n",
        "## Normalize Rebound data\n",
        "reb_scaler = preprocessing.StandardScaler().fit(reb_train_data)\n",
        "rtr_norm = reb_scaler.transform(reb_train_data)\n",
        "rte_norm = reb_scaler.transform(reb_test_data)\n",
        "\n",
        "## Normalize efficiency data\n",
        "eff_scaler = preprocessing.StandardScaler().fit(eff_train_data)\n",
        "etr_norm = eff_scaler.transform(eff_train_data)\n",
        "ete_norm = eff_scaler.transform(eff_test_data)"
      ],
      "metadata": {
        "id": "9dUdBChRmKxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be763395-10fc-4e7d-afcd-b44dbd64eb8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Research\n",
        "\n",
        "Put your code and your experiments here."
      ],
      "metadata": {
        "id": "Hc7HMmNPR10W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here... don't forget to include graphs. Professor Urness loves graphs.\n",
        "\n",
        "\n",
        "# Using Random Forests\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import metrics\n",
        "\n",
        "print(\"Random Forests: \")\n",
        "## Offense\n",
        "off_forest = RandomForestRegressor(random_state = 0, n_estimators=1000)\n",
        "off_forest.fit(otr_norm,off_train_sln)\n",
        "off_predictions = off_forest.predict(ote_norm)\n",
        "\n",
        "off_forest_rs2= metrics.r2_score(off_test_sln, off_predictions)\n",
        "print(\"Off R2: \", off_forest_rs2)\n",
        "\n",
        "## Defense\n",
        "def_forest = RandomForestRegressor(random_state = 0, n_estimators=1000)\n",
        "def_forest.fit(dtr_norm,def_train_sln)\n",
        "def_predictions = def_forest.predict(dte_norm)\n",
        "\n",
        "def_forest_rs2= metrics.r2_score(def_test_sln, def_predictions)\n",
        "print(\"Def R2: \", def_forest_rs2)\n",
        "\n",
        "## Rebounding\n",
        "reb_forest = RandomForestRegressor(random_state = 0, n_estimators=1000)\n",
        "reb_forest.fit(rtr_norm,reb_train_sln)\n",
        "reb_predictions = reb_forest.predict(rte_norm)\n",
        "\n",
        "reb_forest_rs2= metrics.r2_score(reb_test_sln, reb_predictions)\n",
        "print(\"Reb R2: \", reb_forest_rs2)\n",
        "\n",
        "## Efficiency\n",
        "eff_forest = RandomForestRegressor(random_state = 0, n_estimators=1000)\n",
        "eff_forest.fit(etr_norm,eff_train_sln)\n",
        "eff_predictions = eff_forest.predict(ete_norm)\n",
        "\n",
        "eff_forest_rs2= metrics.r2_score(eff_test_sln, eff_predictions)\n",
        "print(\"Eff R2: \", eff_forest_rs2)\n",
        "\n",
        "\n",
        "# Using SGD (code taken and modified from day 9 lecture code)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "sgd = SGDRegressor()\n",
        "\n",
        "print(\"SGD: \")\n",
        "## Offense\n",
        "sgd.fit(otr_norm, off_train_sln)\n",
        "off_sgd_pred = sgd.predict(ote_norm)\n",
        "off_sgd_r2= metrics.r2_score(off_test_sln, off_sgd_pred)\n",
        "print(\"Off R2:\", off_sgd_r2)\n",
        "\n",
        "## Defense\n",
        "sgd.fit(dtr_norm, def_train_sln)\n",
        "def_sgd_pred = sgd.predict(dte_norm)\n",
        "def_sgd_r2= metrics.r2_score(def_test_sln, def_sgd_pred)\n",
        "print(\"Def R2:\", def_sgd_r2)\n",
        "\n",
        "## Rebounding\n",
        "sgd.fit(rtr_norm, reb_train_sln)\n",
        "reb_sgd_pred = sgd.predict(rte_norm)\n",
        "reb_sgd_r2= metrics.r2_score(reb_test_sln, reb_sgd_pred)\n",
        "print(\"Reb R2:\", reb_sgd_r2)\n",
        "\n",
        "## Efficiency\n",
        "sgd.fit(etr_norm, eff_train_sln)\n",
        "eff_sgd_pred = sgd.predict(ete_norm)\n",
        "eff_sgd_r2= metrics.r2_score(eff_test_sln, eff_sgd_pred)\n",
        "print(\"Eff R2:\", eff_sgd_r2)\n",
        "\n",
        "\n",
        "\n",
        "# Using MLP (code taken and modified from day 10 lecture code)\n",
        "\n",
        "print(\"MLP: \")\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "## Offense\n",
        "off_mlp = MLPRegressor(random_state=0,hidden_layer_sizes = (100,), max_iter = 1000)\n",
        "off_mlp.fit(otr_norm,off_train_sln)\n",
        "off_mlp_pred = off_mlp.predict(ote_norm)\n",
        "off_mlp_r2= metrics.r2_score(off_test_sln, off_mlp_pred)\n",
        "print(\"Off R2:\", off_mlp_r2)\n",
        "\n",
        "##Defense\n",
        "def_mlp = MLPRegressor(random_state=0,hidden_layer_sizes = (100,), max_iter = 1000)\n",
        "def_mlp.fit(dtr_norm,def_train_sln)\n",
        "def_mlp_pred = def_mlp.predict(dte_norm)\n",
        "def_mlp_r2= metrics.r2_score(def_test_sln, def_mlp_pred)\n",
        "print(\"Def R2:\", def_mlp_r2)\n",
        "\n",
        "## Rebounding\n",
        "reb_mlp = MLPRegressor(random_state=0,hidden_layer_sizes = (100,), max_iter = 1000)\n",
        "reb_mlp.fit(rtr_norm,reb_train_sln)\n",
        "reb_mlp_pred = reb_mlp.predict(rte_norm)\n",
        "reb_mlp_r2= metrics.r2_score(reb_test_sln, reb_mlp_pred)\n",
        "print(\"Reb R2:\", reb_mlp_r2)\n",
        "\n",
        "## Efficiency\n",
        "eff_mlp = MLPRegressor(random_state=0,hidden_layer_sizes = (100,), max_iter = 1000)\n",
        "eff_mlp.fit(etr_norm,eff_train_sln)\n",
        "eff_mlp_pred = eff_mlp.predict(ete_norm)\n",
        "eff_mlp_r2= metrics.r2_score(eff_test_sln, eff_mlp_pred)\n",
        "print(\"Eff R2:\", eff_mlp_r2)\n",
        "\n",
        "\n",
        "\n",
        "#Graph (used https://www.geeksforgeeks.org/create-a-grouped-bar-plot-in-matplotlib/ for help with creating clustered bar chart)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(4)\n",
        "randforgraph = [off_forest_rs2, def_forest_rs2, reb_forest_rs2, eff_forest_rs2]\n",
        "sgdgraph = [off_sgd_r2, def_sgd_r2, reb_sgd_r2, eff_sgd_r2]\n",
        "mlpgraph = [off_mlp_r2, def_mlp_r2, reb_mlp_r2, eff_mlp_r2]\n",
        "width = 0.2\n",
        "\n",
        "plt.bar(x-0.2, randforgraph, width, color='blue')\n",
        "plt.bar(x, sgdgraph, width, color='red')\n",
        "plt.bar(x+0.2, mlpgraph, width, color='green')\n",
        "plt.xticks(x, ['Off R2', 'Def R2', 'Reb R2', 'Eff R2'])\n",
        "plt.xlabel(\"Statistic\")\n",
        "plt.ylabel(\"R-squared\")\n",
        "plt.legend([\"Rand Forests\", \"SGD\", \"MLP\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XfaACsEOR4U5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "f7317180-d78f-4c8a-a65d-18b1fe6a3a44"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests: \n",
            "Off R2:  0.49757979040908507\n",
            "Def R2:  0.47214691407912446\n",
            "Reb R2:  -0.013461901424794753\n",
            "Eff R2:  0.5089296570221913\n",
            "SGD: \n",
            "Off R2: 0.5479781312437365\n",
            "Def R2: 0.5628729439508766\n",
            "Reb R2: 0.19393120904006167\n",
            "Eff R2: 0.6260590466956967\n",
            "MLP: \n",
            "Off R2: 0.4996138065418453\n",
            "Def R2: 0.5743674571494117\n",
            "Reb R2: 0.13726729467305343\n",
            "Eff R2: 0.6094324738774899\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Analysis\n",
        "\n",
        "What did you discover? What insights/recommendations do you have? What did you find that was interesting? Which model was your best model, which models didn't work well? Why do you think this is? In general, I want a discussion of your experiment, the results, and what they mean."
      ],
      "metadata": {
        "id": "2Qu9bYPLmiv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In all, the main discovery I made by running this project as it relates to the insight I was hoping to gain for NCAA basketball teams is that the most important statistics overall to contribute to winning games are overall offensive and defensive efficiency stats. In every model I ran, the efficiency statistic had greater correlation to the amount of games won than any of the other statistical categories. This makes sense, as it is a more advanced, aggregated stat to measure teams' overall performance. \n",
        "\n",
        "Following this, it appears that defense may be more important to winning games overall than offense. As the classic saying goes, \"defense wins championships\", and this is backed up here. For both of the more accurate models run, SGD and MLP, defense had a higher r-squared value than offense, meaning it is a better predictor of a team's number of wins.\n",
        "\n",
        "A machine learning-related takeaway I have from running this experiment is that the use of the more advanced models we learned in class, Stochastic Gradient Descent and Multilayer Perceptrons, were significantly better predictors than the best of the more basic models we used in the first half of class, random trees. This is logical, as deep learning techniques should theoretically be able to grasp the data better than more basic models. However, I was surprised just how significantly better the deep learning models performed. I think this large gap in prediction ability is the result of deep learning models having much more complex processes through the use of many layers and perceptrons. This allows the algorithm to gain a fuller grasp on the data's whole features, rather than only focusing on individual data inputs."
      ],
      "metadata": {
        "id": "k17sKBZUmqIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Bumps in the Road\n",
        "What challenges did you encounter? How did you overcome these challenges?"
      ],
      "metadata": {
        "id": "TemAuKxlm6dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The biggest challenge I faced throughout this project was determining what dataset I would like to use. The process of choosing a dataset was much more involved than I expected, as I hoped to find a dataset that was mostly or entirely complete with no missing values, and was useful for trying some deep learning methods that we discussed in the second half of this class. I eventually settled on using an older rendition of this college basketball dataset, as it was something I was interested in evaluating, and the older sets were not marred by inconsistencies and missing data as were newer ones throughout the time of the COVID pandemic. I ultimately think I made a good choice, as I did not face any significant issues when using this dataset; it was much more useable than the set I used on my previous project. \n",
        "\n",
        "The other major challenge I faced was figuring out how I wanted to visualize my data, and how I would be able to do so. I knew that I wanted to seperate out the different statistical categories, as the main point I wanted to evaluate was which statistical category was the best predictor of success in games. However, since I used multiple models to gather predictions, I also needed a way to keep those seperated. Therefore, it made logical sense to use a clustered column bar chart, but I had never created one in Python before. I was eventually able to use documentation from geeksforgeeks.org to help me through outlining the process of making this type of chart, then tweaked the documented code and used my inputs to create an effective chart. "
      ],
      "metadata": {
        "id": "zXEVEG9FnHgQ"
      }
    }
  ]
}